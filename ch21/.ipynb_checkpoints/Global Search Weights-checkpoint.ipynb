{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import differential_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on dataset\n",
    "def fit_model(trainX, trainY):\n",
    "    trainY_enc = to_categorical(trainY)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    # fit model\n",
    "    model.fit(trainX, trainY_enc, epochs=500, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, weights, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "    # weighted sum across ensemble members\n",
    "    summed = tensordot(yhats, weights, axes=((0), (0)))\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testY):\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(members, weights, testX)\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testY, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if results == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for optimization process, designed to be minimized\n",
    "def loss_function(weights, members, testX, testY):\n",
    "    # normalize weights\n",
    "    normalized = normalize(weights)\n",
    "    # calculate error rate\n",
    "    return 1.0 - evaluate_ensemble(members, normalized, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, Y = make_blobs(n_samples=1100, centers=3, n_features=2,\n",
    "                 cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, ;], X[n_train:, :]\n",
    "trainY, testY = Y[:n_train], Y[n_train:]\n",
    "\n",
    "# fit all models\n",
    "n_members = 5\n",
    "members = [fit_model(trainX, trainY) for _ in range(n_members)]\n",
    "\n",
    "# evaluate each single model on the test set\n",
    "testY_enc = to_categorical(testY)\n",
    "for i in range(n_members):\n",
    "    _, test_acc = members[i].evaluate(testX, testY_enc, verbose=0)\n",
    "    print('Model %d: %.3f' % (i+1, test_acc))\n",
    "    \n",
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0 / n_members for _ in range(n_members)]\n",
    "score = evaluate_ensemble(members, weights, testX, testY)\n",
    "print('Equal Weights Score: %.3f' % score)\n",
    "\n",
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0) for _ in range(n_members)]\n",
    "\n",
    "# arguments to the loss function\n",
    "search_arg = (members, testX, testY)\n",
    "\n",
    "# global optimization of ensemble weights\n",
    "result = differential_evolution(loss_function, bound_w, search_arg,\n",
    "                               maxiter=1000, tol=1e-7)\n",
    "\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "print('Optimized Weights: %s' % weights)\n",
    "\n",
    "# evaluate chosen weights\n",
    "score = evaluate_ensemble(members, weights, testX, testY)\n",
    "print('Optimized Weights Score: %.3f' % score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
